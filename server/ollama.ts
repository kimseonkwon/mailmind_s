import type { ExtractedEvent } from "@shared/schema";
import { type RagSearchResult } from "@shared/schema";

const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || "http://localhost:11434";

interface OllamaMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface OllamaResponse {
  model: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
}

// ì„ë² ë”© ìƒì„± í•¨ìˆ˜
export async function generateEmbedding(text: string): Promise<number[]> {
  try {
    const cleanText = text.replace(/\n/g, " ");
    
    const response = await fetch(`${OLLAMA_BASE_URL}/api/embeddings`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "nomic-embed-text",
        prompt: cleanText,
      }),
    });

    if (!response.ok) {
      throw new Error(`Embedding API error: ${response.status}`);
    }

    const data = await response.json();
    return data.embedding; 
  } catch (error) {
    console.error("Embedding generation error:", error);
    return [];
  }
}

// ê¸°ë³¸ ì±„íŒ… í•¨ìˆ˜
export async function chatWithOllama(
  messages: OllamaMessage[],
  model: string = "llama3" 
): Promise<string> {
  try {
    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model,
        messages,
        stream: false,
        options: {
          temperature: 0.1, // ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ì°½ì˜ì„± ì–µì œ
        }
      }),
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status}`);
    }

    const data: OllamaResponse = await response.json();
    return data.message.content;
  } catch (error) {
    console.error("Ollama chat error:", error);
    throw new Error("AI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.");
  }
}

// [í•µì‹¬ ìˆ˜ì •] RAG í”„ë¡¬í”„íŠ¸ê°€ ê°•ë ¥í•˜ê²Œ ì ìš©ëœ í•¨ìˆ˜
export async function chatWithEmailContext(
  userQuestion: string,
  retrievedChunks: RagSearchResult[]
): Promise<string> {
  
  // 1. ì°¸ê³ ìë£Œ í¬ë§·íŒ… (ë‚ ì§œ ì •ë³´ ì¶”ê°€, ê°€ë…ì„± ê°œì„ )
  const contextText = retrievedChunks.map((chunk, index) => `
[[ìë£Œ ${index + 1}]]
- Mail ID: ${chunk.mailId}
- ì œëª©: ${chunk.subject}
- ë‚´ìš©: "${chunk.content.replace(/\n/g, " ").replace(/"/g, "'")}"
`).join("\n");

  // 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ ê°•ì œ ë° ì¶œì²˜ í‘œê¸° ê°•í™”)
  const SYSTEM_PROMPT = `
You are a highly intelligent secretary for a Korean user. 
Your task is to answer the user's question based *strictly* on the provided [ì°¸ê³ ìë£Œ] (Reference Materials).

### ğŸš¨ CRITICAL RULES (MUST FOLLOW)
1. **LANGUAGE**: You MUST answer in **Korean (í•œêµ­ì–´)**. Never use English in the final output.
2. **EVIDENCE**: When you state a fact, append the source mail ID.
   - Format: "ì‚¬ì‹¤ ë‚´ìš© (ë©”ì¼ ID: 12)"
3. **NO HALLUCINATION**: If the answer is not in the [ì°¸ê³ ìë£Œ], say "ì œê³µëœ ë©”ì¼ ë‚´ìš©ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
4. **VERIFICATION**: Check if the reference actually answers the specific question. If the topic matches but the specific detail is missing, say so.

### ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
- ë¹„ì¦ˆë‹ˆìŠ¤ ë§¤ë„ˆë¥¼ ê°–ì¶˜ ì •ì¤‘í•œ í•œêµ­ì–´(í•´ìš”ì²´)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ì„œë¡ ("ì°¸ê³ ìë£Œì— ë”°ë¥´ë©´...")ì„ ì¤„ì´ê³ , í•µì‹¬ ê²°ë¡ ë¶€í„° ë§í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ë©”ì¼ì˜ ì •ë³´ê°€ ì„ì—¬ìˆë‹¤ë©´, í•­ëª©ë³„ë¡œ ë‚˜ëˆ„ì–´ ì •ë¦¬í•˜ì„¸ìš”.

### ì˜ˆì‹œ
ì‚¬ìš©ì: "ë‹¤ìŒ ì£¼ íšŒì˜ ì¼ì • ì•Œë ¤ì¤˜"
AI: "ë‹¤ìŒ ì£¼ íšŒì˜ ì¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- **ê²½ì˜ ì „ëµ íšŒì˜**: 10ì›” 5ì¼ ì˜¤í›„ 2ì‹œ, ëŒ€íšŒì˜ì‹¤ (ë©”ì¼ ID: 5)
- **ê°œë°œ íŒ€ ë¯¸íŒ…**: 10ì›” 7ì¼ ì˜¤ì „ 10ì‹œ (ë©”ì¼ ID: 8)"
`;

  // 3. ë©”ì‹œì§€ êµ¬ì„±
  const messages: OllamaMessage[] = [
    { role: "system", content: SYSTEM_PROMPT },
    { 
      role: "user", 
      content: `
[ì°¸ê³ ìë£Œ]
${contextText || "ê´€ë ¨ëœ ë©”ì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

[ì§ˆë¬¸]
${userQuestion}

[ì§€ì¹¨]
ìœ„ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. ê° ì •ë³´ì˜ ì¶œì²˜(ë©”ì¼ ID)ë¥¼ ë°˜ë“œì‹œ í‘œê¸°í•˜ì„¸ìš”.` 
    }
  ];

  return chatWithOllama(messages);
}

export async function checkOllamaConnection(): Promise<boolean> {
  try {
    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);
    return response.ok;
  } catch {
    return false;
  }
}

// ê¸°ì¡´ ë¶„ë¥˜ í•¨ìˆ˜ ìœ ì§€
export async function classifyEmail(
  subject: string,
  body: string,
  sender: string
): Promise<{ classification: string; confidence: string }> {
  const systemPrompt = `ë‹¹ì‹ ì€ ì´ë©”ì¼ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- reference: ë‹¨ìˆœ ì°¸ì¡°
- reply_needed: íšŒì‹  í•„ìš”
- urgent_reply: ê¸´ê¸‰ íšŒì‹ 
- meeting: íšŒì˜
JSON ì‘ë‹µ: {"classification": "meeting", "confidence": "high"}`;

  const userPrompt = `ë°œì‹ ì: ${sender}\nì œëª©: ${subject}\në‚´ìš©: ${body.substring(0, 500)}`;

  try {
    const response = await chatWithOllama([
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ]);

    const jsonMatch = response.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      return JSON.parse(jsonMatch[0]);
    }
    return { classification: "reference", confidence: "low" };
  } catch (error) {
    return { classification: "reference", confidence: "low" };
  }
}